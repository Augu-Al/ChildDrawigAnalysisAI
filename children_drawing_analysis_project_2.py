# -*- coding: utf-8 -*-
"""children_drawing_analysis project#2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHcLAShAMmocW49IDEv2AP--gcPwfGV5
"""

# Install necessary libraries
!pip install torch torchvision opencv-python matplotlib albumentations efficientnet-pytorch wandb

# Mount Google Drive if necessary
from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import os
import torch
import torchvision
from torchvision import transforms, datasets
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import albumentations as A
from albumentations.pytorch import ToTensorV2
from efficientnet_pytorch import EfficientNet
import wandb
import numpy as np
from sklearn.metrics import classification_report

# Initialize Weights & Biases
wandb.init(project="children_drawing_analysis")

# Define the image transformations with Albumentations
transform = A.Compose([
    A.Resize(128, 128),
    A.RandomCrop(120, 120),
    A.HorizontalFlip(p=0.5),
    A.Rotate(limit=15),
    A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),
    A.Normalize(mean=(0.5,), std=(0.5,)),
    ToTensorV2()
])

# Define a dataset class to use Albumentations transforms
class AlbumentationsDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        image = np.array(image)
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented['image']
        return image, label

# Load the dataset
data_dir = '/content/drive/MyDrive/data'
dataset = datasets.ImageFolder(data_dir)

# Split the dataset into training and test sets
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

train_dataset = AlbumentationsDataset(train_dataset, transform=transform)
test_dataset = AlbumentationsDataset(test_dataset, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Use a pre-trained EfficientNet model for transfer learning
model = EfficientNet.from_pretrained('efficientnet-b0')
num_ftrs = model._fc.in_features
model._fc = nn.Linear(num_ftrs, 4)
# Fine-tune the entire model
for param in model.parameters():
    param.requires_grad = True

# Define the criterion, optimizer, and learning rate scheduler
criterion = nn.CrossEntropyLoss()
optimizer = optim.AdamW(model.parameters(), lr=0.0001)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)

# Track experiments with Weights & Biases
wandb.watch(model, log="all")

# Define the training loop with early stopping
num_epochs = 50
best_val_loss = float('inf')
patience, trials = 30, 0

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for inputs, labels in test_loader:
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

    val_loss /= len(test_loader)
    running_loss /= len(train_loader)
    print(f"Epoch {epoch+1}, Training Loss: {running_loss}, Validation Loss: {val_loss}")

    scheduler.step(val_loss)
    wandb.log({"training_loss": running_loss, "validation_loss": val_loss})

    if val_loss < best_val_loss:
        best_val_loss = val_loss
        trials = 0
    else:
        trials += 1
        if trials >= patience:
            print("Early stopping triggered.")
            break

print("Training complete.")

# Evaluate the model
model.eval()
all_labels = []
all_preds = []
correct_predictions = 0
total_images = 0

with torch.no_grad():
    for inputs, labels in test_loader:
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        all_labels.extend(labels.numpy())
        all_preds.extend(preds.numpy())

        for i in range(len(labels)):
            print(f"Image {i+1}:")
            print(f"Ground Truth: {dataset.classes[labels[i]]}")
            print(f"Predicted: {dataset.classes[preds[i]]}")
            print("--------")

        correct_predictions += torch.sum(preds == labels).item()
        total_images += labels.size(0)

accuracy = correct_predictions / total_images
report = classification_report(all_labels, all_preds, target_names=dataset.classes)

print(f"Overall Accuracy: {accuracy * 100:.2f}%")
print("Classification Report:")
print(report)