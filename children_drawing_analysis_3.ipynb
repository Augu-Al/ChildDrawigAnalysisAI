{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch torchvision opencv-python matplotlib albumentations efficientnet-pytorch wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5hcX0WMwNv3",
        "outputId": "385b485e-8778-4077-db52-1181c8a7b357"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Collecting efficientnet-pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.10.3)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=e06f83b1d40411ca91611dc201f53e8891360be04bca23712b64362780e87a2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqUQyKvcwAGa",
        "outputId": "0fec69cf-3de3-4a8b-95b1-7d1cb1c11006"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 1.4.22 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import wandb\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zK1HlyJwxHyD",
        "outputId": "b3593738-ce6a-4e49-f5f8-836093b10f97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Image Transformations"
      ],
      "metadata": {
        "id": "E3lhyufTgEV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights & Biases\n",
        "wandb.init(project=\"children_drawing_analysis\")\n",
        "\n",
        "# the image transformations with Albumentations\n",
        "transform = A.Compose([\n",
        "    A.Resize(128, 128),\n",
        "    A.RandomCrop(120, 120),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.Rotate(limit=10),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "uc4oyp5pf-Rk",
        "outputId": "c284f138-5979-46cd-821f-2dfb1fedc26a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241208_011640-qgvsv3s5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/alenaugustine96-fanshawe-college/children_drawing_analysis/runs/qgvsv3s5' target=\"_blank\">wandering-feather-19</a></strong> to <a href='https://wandb.ai/alenaugustine96-fanshawe-college/children_drawing_analysis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/alenaugustine96-fanshawe-college/children_drawing_analysis' target=\"_blank\">https://wandb.ai/alenaugustine96-fanshawe-college/children_drawing_analysis</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/alenaugustine96-fanshawe-college/children_drawing_analysis/runs/qgvsv3s5' target=\"_blank\">https://wandb.ai/alenaugustine96-fanshawe-college/children_drawing_analysis/runs/qgvsv3s5</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Dataset Class"
      ],
      "metadata": {
        "id": "f0egBwSVgQGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset class to use Albumentations transforms\n",
        "class AlbumentationsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.dataset[idx]\n",
        "        image = np.array(image)\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "Q5J9eRpbgMN5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "Mz4-VleJgfr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/data'\n",
        "dataset = datasets.ImageFolder(data_dir)\n",
        "\n",
        "# Split the dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "train_dataset = AlbumentationsDataset(train_dataset, transform=transform)\n",
        "test_dataset = AlbumentationsDataset(test_dataset, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "epzkv1zVgag5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Pre-trained Model and Modify"
      ],
      "metadata": {
        "id": "zSbbE5N2gycI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "num_ftrs = model._fc.in_features\n",
        "model._fc = nn.Linear(num_ftrs, 4)\n",
        "model.dropout = nn.Dropout(p=0.5)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V5DSG1AgjWV",
        "outputId": "402fdf50-4301-4f4a-9c5f-ed455fdb67ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
            "100%|██████████| 20.4M/20.4M [00:00<00:00, 80.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Loss Function, Optimizer, Scheduler,training Loop and evaluating the model."
      ],
      "metadata": {
        "id": "qi4xGEabg_Aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the criterion, optimizer, and learning rate scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "# Track with Weights & Biases\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "# early stopping and model checkpointing\n",
        "num_epochs = 50\n",
        "best_val_loss = float('inf')\n",
        "patience, trials = 30, 0\n",
        "best_model_path = 'best_model.pt'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    running_loss /= len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss}, Validation Loss: {val_loss}\")\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "    wandb.log({\"training_loss\": running_loss, \"validation_loss\": val_loss})\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        trials = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "    else:\n",
        "        trials += 1\n",
        "        if trials >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(\"Training complete.\")\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "# evaluate the model\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "correct_predictions = 0\n",
        "total_images = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_preds.extend(preds.numpy())\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            print(f\"Image {i+1}:\")\n",
        "            print(f\"Ground Truth: {dataset.classes[labels[i]]}\")\n",
        "            print(f\"Predicted: {dataset.classes[preds[i]]}\")\n",
        "            print(\"--------\")\n",
        "\n",
        "        correct_predictions += torch.sum(preds == labels).item()\n",
        "        total_images += labels.size(0)\n",
        "\n",
        "accuracy = correct_predictions / total_images\n",
        "report = classification_report(all_labels, all_preds, target_names=dataset.classes)\n",
        "\n",
        "print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUa2wUrWgxGS",
        "outputId": "b5e67cbb-aa54-4147-d83f-7044db02cd30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 1.375597443845537, Validation Loss: 1.3538201808929444\n",
            "Epoch 2, Training Loss: 1.2499527467621698, Validation Loss: 1.323795747756958\n",
            "Epoch 3, Training Loss: 1.13219177391794, Validation Loss: 1.263758659362793\n",
            "Epoch 4, Training Loss: 1.02785904539956, Validation Loss: 1.2199821829795838\n",
            "Epoch 5, Training Loss: 0.950058745013343, Validation Loss: 1.1460837841033935\n",
            "Epoch 6, Training Loss: 0.8629686236381531, Validation Loss: 1.1268176317214966\n",
            "Epoch 7, Training Loss: 0.7837329275078244, Validation Loss: 1.0841444611549378\n",
            "Epoch 8, Training Loss: 0.7095690766970316, Validation Loss: 1.0336975812911988\n",
            "Epoch 9, Training Loss: 0.6420399331384234, Validation Loss: 0.9514254331588745\n",
            "Epoch 10, Training Loss: 0.5911740511655807, Validation Loss: 0.9686207890510559\n",
            "Epoch 11, Training Loss: 0.49231422775321537, Validation Loss: 1.0022237181663514\n",
            "Epoch 12, Training Loss: 0.4974269883500205, Validation Loss: 0.9823102474212646\n",
            "Epoch 13, Training Loss: 0.4387323442432616, Validation Loss: 0.9621938109397888\n",
            "Epoch 14, Training Loss: 0.3676883942551083, Validation Loss: 0.8729692459106445\n",
            "Epoch 15, Training Loss: 0.33721938067012364, Validation Loss: 0.9333487987518311\n",
            "Epoch 16, Training Loss: 0.2993398714396689, Validation Loss: 0.9692292451858521\n",
            "Epoch 17, Training Loss: 0.2580263026886516, Validation Loss: 0.989087951183319\n",
            "Epoch 18, Training Loss: 0.22349133673641416, Validation Loss: 1.0089742183685302\n",
            "Epoch 19, Training Loss: 0.21980505685011545, Validation Loss: 0.9401211619377137\n",
            "Epoch 20, Training Loss: 0.17578284897738033, Validation Loss: 0.9455657958984375\n",
            "Epoch 21, Training Loss: 0.16515515247980753, Validation Loss: 1.0325753211975097\n",
            "Epoch 22, Training Loss: 0.1599019327097469, Validation Loss: 0.9138488531112671\n",
            "Epoch 23, Training Loss: 0.1601797396110164, Validation Loss: 0.8981946587562561\n",
            "Epoch 24, Training Loss: 0.158495447701878, Validation Loss: 0.8910111606121063\n",
            "Epoch 25, Training Loss: 0.1734466581708855, Validation Loss: 0.9954230308532714\n",
            "Epoch 26, Training Loss: 0.13777268222636646, Validation Loss: 1.0486482977867126\n",
            "Epoch 27, Training Loss: 0.1468860465619299, Validation Loss: 0.9611350774765015\n",
            "Epoch 28, Training Loss: 0.1541016639934646, Validation Loss: 0.9211390078067779\n",
            "Epoch 29, Training Loss: 0.15371496147579616, Validation Loss: 0.948201036453247\n",
            "Epoch 30, Training Loss: 0.15270344581868914, Validation Loss: 0.9443835854530335\n",
            "Epoch 31, Training Loss: 0.1541621937519974, Validation Loss: 0.9594569206237793\n",
            "Epoch 32, Training Loss: 0.12906401107708612, Validation Loss: 0.9606720685958863\n",
            "Epoch 33, Training Loss: 0.12481699676977263, Validation Loss: 0.9703753590583801\n",
            "Epoch 34, Training Loss: 0.16170245160659155, Validation Loss: 0.9868363380432129\n",
            "Epoch 35, Training Loss: 0.15602606824702686, Validation Loss: 0.9656244158744812\n",
            "Epoch 36, Training Loss: 0.16629604746898016, Validation Loss: 0.9650599002838135\n",
            "Epoch 37, Training Loss: 0.16635380892290008, Validation Loss: 0.9771601915359497\n",
            "Epoch 38, Training Loss: 0.14588643486301103, Validation Loss: 0.9404899477958679\n",
            "Epoch 39, Training Loss: 0.15482180648379856, Validation Loss: 0.9505743265151978\n",
            "Epoch 40, Training Loss: 0.1402897048327658, Validation Loss: 1.016358482837677\n",
            "Epoch 41, Training Loss: 0.15139573936661085, Validation Loss: 1.008653211593628\n",
            "Epoch 42, Training Loss: 0.1432481693724791, Validation Loss: 0.9863017439842224\n",
            "Epoch 43, Training Loss: 0.1595960660941071, Validation Loss: 1.0768102288246155\n",
            "Epoch 44, Training Loss: 0.14367801075180373, Validation Loss: 1.0168499588966369\n",
            "Early stopping triggered.\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-56a02c692c86>:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(best_model_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1:\n",
            "Ground Truth: Angry\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 2:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 3:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 4:\n",
            "Ground Truth: Fear\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 5:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 6:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 7:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 8:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 9:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 10:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 11:\n",
            "Ground Truth: Fear\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 12:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 13:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 14:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 15:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 16:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 17:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 18:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 19:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 20:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 21:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 22:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 23:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 24:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 25:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 26:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 27:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 28:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 29:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 30:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 31:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 32:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 1:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 2:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 3:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 4:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 5:\n",
            "Ground Truth: Fear\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 6:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 7:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 8:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 9:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 10:\n",
            "Ground Truth: Sad\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 11:\n",
            "Ground Truth: Angry\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 12:\n",
            "Ground Truth: Fear\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 13:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 14:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 15:\n",
            "Ground Truth: Sad\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 16:\n",
            "Ground Truth: Sad\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 17:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 18:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 19:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 20:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 21:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 22:\n",
            "Ground Truth: Fear\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 23:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 24:\n",
            "Ground Truth: Sad\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 25:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 26:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 27:\n",
            "Ground Truth: Happy\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 28:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 29:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 30:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 31:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 32:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 1:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 2:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 3:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 4:\n",
            "Ground Truth: Happy\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 5:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 6:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 7:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 8:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 9:\n",
            "Ground Truth: Fear\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 10:\n",
            "Ground Truth: Fear\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 11:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 12:\n",
            "Ground Truth: Fear\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 13:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 14:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 15:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 16:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 17:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 18:\n",
            "Ground Truth: Sad\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 19:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 20:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 21:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 22:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 23:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 24:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 25:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 26:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 27:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 28:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 29:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 30:\n",
            "Ground Truth: Fear\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 31:\n",
            "Ground Truth: Angry\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 32:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 1:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 2:\n",
            "Ground Truth: Sad\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 3:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 4:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 5:\n",
            "Ground Truth: Sad\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 6:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 7:\n",
            "Ground Truth: Happy\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 8:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 9:\n",
            "Ground Truth: Fear\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 10:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 11:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 12:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 13:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 14:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 15:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 16:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 17:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 18:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 19:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 20:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 21:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 22:\n",
            "Ground Truth: Angry\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 23:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 24:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 25:\n",
            "Ground Truth: Sad\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 26:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 27:\n",
            "Ground Truth: Sad\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 28:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 29:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 30:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 31:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 32:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 1:\n",
            "Ground Truth: Fear\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 2:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 3:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 4:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 5:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 6:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 7:\n",
            "Ground Truth: Fear\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 8:\n",
            "Ground Truth: Sad\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 9:\n",
            "Ground Truth: Happy\n",
            "Predicted: Happy\n",
            "--------\n",
            "Image 10:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Image 11:\n",
            "Ground Truth: Angry\n",
            "Predicted: Angry\n",
            "--------\n",
            "Image 12:\n",
            "Ground Truth: Sad\n",
            "Predicted: Fear\n",
            "--------\n",
            "Image 13:\n",
            "Ground Truth: Angry\n",
            "Predicted: Sad\n",
            "--------\n",
            "Overall Accuracy: 63.83%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       0.62      0.55      0.58        42\n",
            "        Fear       0.55      0.47      0.51        36\n",
            "       Happy       0.79      0.90      0.84        29\n",
            "         Sad       0.60      0.71      0.65        34\n",
            "\n",
            "    accuracy                           0.64       141\n",
            "   macro avg       0.64      0.66      0.64       141\n",
            "weighted avg       0.63      0.64      0.63       141\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Children's Drawing Analysis Report"
      ],
      "metadata": {
        "id": "JrbRL6j6FT0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Overview"
      ],
      "metadata": {
        "id": "rH5KpihQFYv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project I aimed to classify drawings using a deep learning model. The EfficientNet pre-trained model was fine-tuned on a dataset of images, which were augmented using Albumentations to enhance generalization."
      ],
      "metadata": {
        "id": "qlKt3Y-gGACd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing\n"
      ],
      "metadata": {
        "id": "dTz0mIfGGCE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was preprocessed with various transformations: resizing images to 128x128 pixels, randomly cropping to 120x120 pixels, applying horizontal flips, rotations, color jittering, and normalization to standardize the pixel values."
      ],
      "metadata": {
        "id": "mNGQn2tWHWOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset was sourced from kaggle(https://www.kaggle.com/datasets/vishmiperera/children-drawings/data), containing a total of len 702 images. The dataset was split into training and test sets, with len 561 images for training and len 141 for testing. The dataset included multiple classes, such as\n",
        " Angry, Fear, Happy, Sad."
      ],
      "metadata": {
        "id": "04uQRqtjHz41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The EfficientNet-b0 architecture was modified by replacing the final fully connected layer to classify into four classes. Dropout with a probability of 0.5 was added to prevent overfitting. The model was fine-tuned with the entire network's parameters set to be trainable."
      ],
      "metadata": {
        "id": "l-WJzsqOUp-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training process involved using CrossEntropyLoss as the criterion for handling multi-class classification. The AdamW optimizer, with a learning rate of 0.0001, was employed due to its adaptability to sparse gradients. A learning rate scheduler, ReduceLROnPlateau, was used to reduce the learning rate by a factor of 0.1 if the validation loss did not improve for five epochs. The model was trained for a maximum of 50 epochs with a batch size of 32. Early stopping was implemented with a patience of 30 trials, ensuring training stopped early if the validation loss did not improve. Model checkpointing was used to save the best model based on validation loss."
      ],
      "metadata": {
        "id": "NRwF9NwZUsyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Throughout the training process, Weights & Biases was used to log experiments, allowing detailed tracking of model performance and hyperparameters. Performance metrics such as training loss and validation loss were monitored. The lowest validation loss achieved during training, noted as best_val_loss, was recorded."
      ],
      "metadata": {
        "id": "7RWHqxwXVHpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, the model achieved an accuracy of accuracy 61.70% on the test set, indicating a good performance in classifying children's drawings. The classification report provided detailed insights into the model's performance across different categories, including precision, recall, and F1-score for each class."
      ],
      "metadata": {
        "id": "eKESrY3wVWAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function, Optimizer, and Scheduler used here:\n",
        "\n",
        "CrossEntropyLoss is a commonly used loss function for classification tasks. It combines LogSoftmax and NLLLoss in one single class. The input to CrossEntropyLoss is the raw scores (logits) from the model, and it outputs a loss value which indicates how far the model's predictions are from the true labels\n",
        "\n",
        "AdamW is an optimization algorithm that adjusts the learning rate of each parameter. It’s a variant of the Adam optimizer with weight decay to improve generalization. AdamW helps in minimizing the loss function by adjusting the model parameters based on their gradients.\n",
        "Learning Rate: Controls how much to change the model parameters in response to the estimated error each time the model weights are updated. A smaller learning rate like 0.0001 makes the training more stable by making small adjustments.\n",
        "\n",
        "ReduceLROnPlateau is a learning rate scheduler that adjusts the learning rate based on the validation loss. When the validation loss stops improving, this scheduler reduces the learning rate by a factor of 0.1. This helps in fine-tuning the learning rate, allowing the model to settle into minima during training.\n",
        "\n",
        "Mode: 'min' mode means the scheduler looks for a decrease in the validation loss to decide whether to reduce the learning rate.\n",
        "\n",
        "Factor: The factor by which the learning rate will be reduced. Here, it’s set to 0.1.\n",
        "\n",
        "Patience: The number of epochs with no improvement after which the learning rate will be reduced. It’s set to 5 here."
      ],
      "metadata": {
        "id": "POvhB37kh8NK"
      }
    }
  ]
}